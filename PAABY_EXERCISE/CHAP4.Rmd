---
title: "CHAPTER 4"
author: "Laura W. Paaby"
date: "2/9/2022"
output: html_document
---
```{r}
pacman::p_load(tidyverse, rethinking)
```

# THE FLIP COIN EXPERIMENT ON THE SOCCER FIELD
all your friends stand at the sideline of a soccer field, and flip a coin 16 times - if one side they go left, if the other they go right. 
even though the outcome is binomial, the distribution will still be Gaussian - lets see why:
```{r}
 pos <- replicate( 1000 , sum( runif(16,-1,1) ) )
####we generate for each person a list of 16 random numbers between −1 and 1. These are the individual steps. Then we add these steps together to get the position after 16 steps. Then we need to replicate this procedure 1000 times. 

hist(pos)
dens(pos)
```

### normal by multiplacation
```{r}
prod( 1 + runif(12,0,0.1) )
```
This code just samples 12 random numbers between 1.0 and 1.1, each representing a pro- portional increase in growth. Thus 1.0 means no additional growth and 1.1 means a 10% increase. The product of all 12 is computed and returned as output. Now what distribution do you think these random products will take?
```{r}
growth <- replicate( 10000 , prod( 1 + runif(12,0,0.1) ) ) 
dens( growth , norm.comp=TRUE )
```

Again still obviously true - now not because of fluctation in addition, but because the effect is small, and multiplying small effects, is kinda like addition: 
```{r}
big <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) ) 
small <- replicate( 10000 , prod( 1 + runif(12,0,0.01) ) )

dens( big , norm.comp=TRUE )
dens( small , norm.comp=TRUE )
### we verify how the smaller effect is closer a normal distribution here ... -> the smaller the effect the better the approximation 

## this can be fixed by taking the log of the big:
log.big <- replicate( 10000 , log(prod(1 + runif(12,0,0.5))) )
dens( log.big , norm.comp=TRUE ) #WUHU a normal distribution
```
# BUILDING A REGRESSION
```{r}
data(Howell1)
d <- Howell1

precis(d)
### well work with the height
d$height
```
```{r}
# we only want to work with adults, so filter out people under 18:
data <- d[d$age>=18,] ### the , just ensures we get the entire matrix with us :D 
```
We’re ready to write down the general model and compute the plausibility of each combination of μ and σ, which are the one we wants to investigate:
$$ h_i ~ Normal(\mu, \sigma) $$
where i is each individual in the list. (the index for each row number)
SO ALL THE MODEL knows about each height measurement is defined by the same normal distribution, with mean μ and standard deviation σ.!!!

##### making priors:
these are just chosen priors:  
μ = 178
σ = 20

```{r}
 curve( dnorm( x , 178 , 20 ) , from=100 , to=250 ) #this plots the curve from 100-200cm, where 178 is the mean and the standard deviation 20cm. 
```

now that this is done we must make the prior predictive simulation - which is to make priors based on the model, before fitting the data:
```{r}
sample_mu <- rnorm( 1e4 , 178 , 20 )
sample_sigma <- runif( 1e4 , 0 , 50 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma ) 
dens( prior_h )
```
The distribution you see is not an empirical expectation, but rather the distribution of relative plausibilities of different heights, before seeing the data.

OKAY BUT TAKE A LOOK AT THIS - many people are ABOVE 250 cm ... not very precise ...

```{r}
## now the sd is no longer 20 but 100, because the book says so .... 
sample_mu <- rnorm( 1e4 , 178 , 100 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma ) 
dens( prior_h )
```

now the heights are going all crazy ....... 
We will now manually add the posterior distribution by a *grid approximation* ..

## POSTERIOR AND GRID:
```{r}
mu.list <- seq( from=150, to=160 , length.out=100 ) 
sigma.list <- seq( from=7 , to=9 , length.out=100 ) 
post <- expand.grid( mu=mu.list , sigma=sigma.list ) 

post$LL <- sapply( 1:nrow(post), function(i) sum(
  dnorm( data$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) )

post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) + dunif( post$sigma , 0 , 50 , TRUE ) 

post$prob <- exp( post$prod - max(post$prod) )


# PLOTS
contour_xyz( post$mu , post$sigma , post$prob ) #contourmap 
image_xyz( post$mu , post$sigma , post$prob ) #heatmap
```

# sampling from the posterior 
=> this is just like when we previously sampled p, now we just need it for both the mean and sd:
```{r}
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE , prob=post$prob )
sample.mu <- post$mu[ sample.rows ] #samling the mean, from the mu generated
sample.sigma <- post$sigma[ sample.rows ] #sampling the standard deviation from the sigma generated
```
*we have now simulated 10000 samples of the height data, lets take a look at it:*
```{r}
 plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
# cex => character expansion
# pch => plot character
# col.aplha makes it seethrough
```


**LOOKING AT THE FRESHLY MADE SIGMA AND MU**
```{r}
dens( sample.mu, adj = 0.1 ) 
dens( sample.sigma, adj = 0.1 )
## WE SEE KIND OF NORMAL DISTRIBUTIONS DD
```

```{r}
# finding the PI POSTERIOR INTERVALS:
PI(sample.mu)
PI(sample.sigma)
```



# analyzing 20 random heights 
```{r}
data_height <- sample(data$height, size=20)


## making the mu - mean, and sigma - standard deviation:
mu.list <- seq(from=150, to=170, length.out = 200)
sigma.list <- seq(from = 4, to = 20, length.out = 200)

# making the posterior:
post2 <- expand.grid(mu = mu.list, sigma=sigma.list)
?expand.grid #Creates a data frame from all combinations of the supplied vectors or factors¨¨¨

post2$LL <- sapply(1:nrow(post2) , function(i)
    sum( dnorm( data_height , mean=post2$mu[i] , sd=post2$sigma[i] ,log=TRUE ) ) )


post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +
dunif( post2$sigma , 0 , 50 , TRUE )

post2$prob <- exp( post2$prod - max(post2$prod) )
sample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE ,prob=post2$prob )

sample2.mu <- post2$mu[ sample2.rows ] 
sample2.sigma <- post2$sigma[ sample2.rows ] 

plot( sample2.mu , sample2.sigma , cex=0.5 ,col=col.alpha(rangi2,0.1) , xlab="mu" , ylab="sigma" , pch=16 )

```
After executing the code above, you’ll see another scatter plot of the samples from the posterior den- sity, but this time you’ll notice a distinctly longer tail at the top of the cloud of points. You should also inspect the marginal posterior density for σ, averaging over μ, produced with:

```{r}
dens(sample2.sigma, norm.comp = TRUE)
```





# QUADRATIC APPROXIMATION
We do this to find the posteriors maximum peak MAP, which is useful since it tells us something about the shape of the distribution. 

```{r}
# we gonna use:
data #people over 18 only
```
=> OUR MODEL DEFINITION: 
height ~ dnorm(mu,sigma)
mu ~ dnorm(156,10)
sigma ~ dunif(0,50)


whut is alist?!??!?!!? => The two functions *alist* and *list* do the same basic thing: allow you to make a collection of arbitrary R objects. They differ in one important respect: list evaluates the code you embed inside it, while alist does not. So when you define a list of formulas, you should use alist, so the code isn’t ex- ecuted. B
```{r}
flist <- alist(height ~ dnorm( mu , sigma ) ,
               mu ~ dnorm( 178 , 20 ) , 
               sigma ~ dunif( 0 , 50 )
)


#### FITTING THE MODEL - a weird way to do it i feel like ;D
m4_1 <- quap(flist,data = data)  #but hacing this model of the posterior distribution, we can now investigate it further:

precis(m4_1)
```

The 5.5% and 94.5% quantiles are percentile interval boundaries, corresponding to an 89% compatibility interval. it can be changed by:
```{r}
precis(m4_1,prob = 0.95)
```
CHANGING up the priors - so they no longer sooooo weak;
```{r}
m4.2 <- quap( 
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu ~ dnorm( 178 , 0.1 ) , 
    sigma ~ dunif( 0 , 50 )
    ) , data=data ) 

precis( m4.2 )
```
ALL WE HAVE GOTTEN SO FAR IS A *QUADRATIC APPROXIMATION OF THE PRIOR*
=> How do we get to *samples* form quadratic approximate *posterior distribution*????? 
ANSWER: we must recognize that a a quadratic approximation to a posterior distribution with MORE than one parameter dimension (mu and sigma contribute to one dimension), is just a multi dimensional gaussian distribution.....

```{r}
#finding the variance-covariance matrix:
vcov(m4_1)
```

```{r}
diag( vcov( m4_1 ) ) # a vector of variances for the parameters
cov2cor( vcov( m4_1 ) ) # correlation matrix, that tells us how changes in any parameter lead to correlated changes in the others
```
Each entry of the 2x2 matrix in the bottom shows the correlation, bounded between −1 and +1, for each pair of parameters. (if 1, correlation with it self), were the one close to 0 tells us that knowing sigma in this case tells us nothing about mu, and the other way around - cause the numbers are tiny.

NOW it is time for the *multi dimensional sampling*:
```{r}
library(rethinking)
post <- extract.samples( m4_1 , n=1e4 ) 
head(post)
```

```{r}
precis(post) #the quadratic sampled posteriors
precis(m4_1) # the models values 
```



*okay so so far we have made a GAUSSIAN MODEL OF HEIGHT in a adult population .... doesn't really feel like it though :DDDD*

because ...., it doesn't have a predictor variable 

first lets just look how much height and weight covary:
```{r}
plot(data$height ~ data$weight)
```


# LINEAR REGRESsiON
to understand the betha values (parameters) of the linear models, and their distribution as priors - and why this is a gaussian prior with the mean zero (???) we must simulate the *prior predictive distribution*

THE GOAL: to simulate height from the model using only priors:
the information/priors we will use in then the weight . 
```{r}
#making a bunch of lines, implied by the priors for alpha and betha:
set.seed(2971)
N <- 100
a <- rnorm( N , 178 , 20 ) 
b <- rnorm( N , 0 , 10 )


### plotting the lines:
plot( NULL , xlim=range(data$weight) , ylim=c(-100,400) , xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(data$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) , #the linear regression are made for each point here
from=min(data$weight) , to=max(data$weight) , add=TRUE , col=col.alpha("black",0.2) )
```
remember: we have not yet seen the data ... but DUDE this is a bad model :D who the fuck is lower than -100 cm???

Also the increment of height only happens into some value ... we can thus restrict the positive values => this is done by defining the prior as log-normal instead

```{r}
b <- rlnorm( 1e4 , 0 , 1 ) #rlnorm is a log-normal distribution
dens( b , xlim=c(0,5) , adj=0.1 )
```
this makes betha b positive, the reason is that exp(x) is greater than zero for any real number x - thus log priors are common place => they are an easy way to enforce postive relationships 

doing the prior simulation again, but with log-normal priors:
```{r}
set.seed(2971)
N <- 100 # 100 lines a <- rnorm( N , 178 , 20 )
b <- rlnorm( N , 0 , 1 )

dens(b, xlim=c(0,5), adj=0.1)
```



# FINDING THE POSTERIOR DISTRIBUTION
still using the data from Howell1:
```{r}
# define the average weight, x-bar 
xbar <- mean(data$weight)

# fit model 
m4.3 <- quap(
  alist(height ~ dnorm( mu , sigma ) , 
  mu <- a + b*( weight - xbar ) , # mu is now a function of other parameters (a and b), and thus no longer a parameter on its own - it is however still uncertain cause it depends on uncertain parameters
  a ~ dnorm( 178 , 20 ) ,
  b ~ dlnorm( 0 , 1 ) ,
  sigma ~ dunif( 0 , 50 ) ),
data=data )


```
*so what we do is making a model with the quap function - here based on:*
hi ∼ Normal(μi, σ)
μi =α+β(xi − ̄x)
α ∼ Normal(178, 20) 
β ∼ Log-Normal(0, 1)
σ ∼ Uniform(0, 50)

*now the interpretation of this bad boy*:
OBS: once you fit the model it can only report posterior distribution

```{r}
precis(m4.3)
```

The first row gives the quadratic approximation for α, the second the approximation for β, and the third approximation for σ

 Since β is a slope, the value 0.90 can be read as a person 1 kg heavier is expected to be 0.90 cm taller. 89% of the posterior probability lies between 0.84 and 0.97. That suggests that β values close to zero or greatly above one are highly incompatible with these data and this model. It is most certainly not evidence that the relationship between weight and height is linear, because the model only considered lines. It just says that, if you are committed to a line, then lines with a slope around 0.9 are plausible ones.

BUT TTHIS IS NOT ENOUGHB => we must look at the covariance matrix: 
```{r}
round(vcov(m4.3), 3) #covariance
pairs(m4.3) #showing the marginal posterior and the covariance
```

## plotting against the data
```{r}
plot( height ~ weight , data=data, col=rangi2 ) 
post <- extract.samples( m4.3 )
a_map <- mean(post$a)
b_map <- mean(post$b)
curve( a_map + b_map*(x - xbar) , add=TRUE )
```


taking out samples of each possible posterior;
```{r}
post <- extract.samples( m4.3 ) 
post[1:5,]
```


plotting it;

```{r}
N <- 10
dN <- data[ 1:N , ] 
mN <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*( weight - mean(weight) ) , a ~ dnorm( 178 , 20 ) ,
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
) , data=dN )


# extract 20 samples from the posterior post <- extract.samples( mN , n=20 )
# display raw data and sample size
plot( dN$weight , dN$height ,
    xlim=range(data$weight) , ylim=range(data$height) ,
    col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N))
# plot the lines, with transparency
for ( i in 1:20 )
curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) , col=col.alpha("black",0.3) , add=TRUE )

```
```{r}
post <- extract.samples( m4.3 )
mu_at_50 <- post$a + post$b * ( 50 - xbar )

#this is now posting the samples for a person that has the wheight of 50
dens( mu_at_50 , col=rangi2 , lwd=2 , xlab="mu|weight=50" )
```
Since the posterior for μ is a distribution, you can find intervals for it, just like for any posterior distribution. To find the 89% compatibility interval of μ at 50 kg, just use the PI command as usual::
```{r}
PI(mu_at_50, prob=0.89)
```

```{r}
mu <- link( m4.3 ) 
str(mu)
# we have a distribution of μ for each individual in the original data

## BUT We actually want something slightly different: a distribution of μ for each unique weight value on the horizontal axis. It’s only slightly harder to compute that, by just passing link some new data:

# define sequence of weights to compute predictions for # these values will be on the horizontal axis 
weight.seq <- seq( from=25 , to=70 , by=1 )
# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link( m4.3 , data=data.frame(weight=weight.seq) ) 
str(mu)
```
And now there are only 46 columns in mu, because we fed it 46 different values for weight.
To visualize what you’ve got here, let’s plot the distribution of μ values at each height.
```{r}
# use type="n" to hide raw data
plot( height ~ weight , data , type="n" )
# loop over samples and plot each mu value
for ( i in 1:100 )
points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )


# summarize the distribution of mu 
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )


# plot raw data
# fading out points to make line and interval more visible 
plot( height ~ weight , data=data , col=col.alpha(rangi2,0.5) )
# plot the MAP line, aka the mean mu for each weight 
lines( weight.seq , mu.mean )
# plot a shaded region for 89% PI 
shade( mu.PI , weight.seq )
```





### Problems from Chapter 4 of *Statistical Rethinking*.

#### Problem 4E1

4E1: In the model definition below, which line is the likelihood? 
*yi ∼ Normal(μ, σ) (this is the likelihood)*
μ ∼ Normal(0, 10) (μ prior)
σ ∼ Exponential(1) (σ prior)


#### Problem 4E2
combination of posterior and likelihood gives our posterior distribution

4E2. In the model definition just above, how many parameters are in the posterior distribution?
*Two parameters μ(my) and σ(sigma)*

#### Problem 4E3

4E3. Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.

Note for myself: 
µ is the mean of the population
σ is the standard deviation 

$$Pr(\sigma, \mu|y) = \frac{\Pi_i Normal(y_i|\mu \sigma) Normal(\mu|0,10) Uniform(\sigma|1,10)}  {\int \int \Pi_i Normal(y_i|\mu \sigma) Normal(\mu|0,10) Uniform(\sigma|1,10)}$$
side 87 kan man finde formlen der viser det.


#### Problem 4E4

4E4. In the model definition below, which line is the linear model? 
$$y_i ~ normal(\mu, \sigma)$$ 
$$\mu_i = \alpha + \beta x_i$$
this one ^^^^ is the linear 
$$\alpha ~ \sim Normal(0,10)$$

yi ∼ Normal(μ, σ)
*μi =α+βxi*
α ∼ Normal(0, 10) 
β ∼ Normal(0, 1) 
σ ∼ Exponential(2)

The linear model is the μi =α+βxi

#### Problem 4E5

4E5. In the model definition just above, how many parameters are in the posterior distribution?
The model has 3 parameters: 
α, β and σ

#### Problem 4M1

4M1. For the model definition below, simulate observed y values from the prior (not the posterior). 
yi ∼ Normal(μ, σ)
μ ∼ Normal(0, 10) 
σ ∼ Exponential(1)

```{r}
N <- 10000 # numbers of observation
mu <- rnorm(N,0,10) # normalized 0,10 as stated in the model 
sigma <- rexp(N,1) #exponential

y <- rnorm(N, mu, sigma) #simulating y values
dens(y) #plotting it
```

#### Problem 4M2

4M2. Translate the model just above into a quap formula.
yi ∼ Normal(μ, σ)
μ ∼ Normal(0, 10) 
σ ∼ Exponential(1)

```{r}
list_qua <- alist(
    y ~ dnorm(mu, sigma) , 
    mu ~ dnorm(0,10) , 
    sigma ~ dexp(1) 
)

#so the difference here is that we use dnorm and dunif instead of rnorm and runif
#rnorm - return randoms numbers following a normal distribution
#dnorm - follows the probability density function - therefore we use it when transferring into a quap formula 

```

#### Problem 4M3

Translate the quap model formula below into a mathematical model definition.

```{r}
#this is the model which we want to make a mathematical model definition 
flist <- alist(
    y ~ dnorm( mu , sigma ),
    mu <- a + b*x,
    a ~ dnorm( 0 , 10 ),
    b ~ dunif( 0 , 1 ),
    sigma ~ dexp( 1 )
)


#Mathematical model 
#(not r )
y ~ Normal(mu, sigma)
mu = Normal(a+Bxi)
a ~ Normal(0,10)
b ~ Uniform(0,1) 
sigma ~ Exponential(1) 

```

$$y \sim Normal(\mu, \sigma)$$
$$\mu = Normal(\alpha + \beta x_i)$$
$$\alpha \sim Normal(0,10)$$
$$\beta \sim Uniform(0,1)$$ 
$$\sigma \sim Exponential(1)$$


#### Problem 4M4

A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.

µ is the mean of the population
σ is the standard deviation 

x ~ year 
h ~ height
height ~ year

$$h_i \sim Normal(\mu, \sigma) $$
hi ~ Normal(mu, sigma)
$$\mu \sim \alpha + \beta x_i$$
mu ~ a + bxi

$$\alpha \sim Normal(150, 10)$$ 

150 cm is the normal distribution with a sd of 10 cm. - so children are in the range of 130 and 170 

Our beta is 5 cm with a sd of 1 - we expect children to grow alot 
$$\beta \sim Normal(5,1)$$
$$\sigma \sim Exponential(1)$$


```{r}
#showing
hist(rexp(1000, 1))
```

#### Problem 4M5

Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?

When I know that the students get taller every year - it much mean that they are children still growing alot. We did take this into account in our model. 

#### Problem 4M6
Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?

Hm i then would change my a 
a ~ Normal(135, 32) given a range of 64 cm variance okay this was my first thought 
online they say that its sigma we will change when knowing the overall variance we can take the square root of it
```{r}
sqrt(64)
```
$$\alpha \sim Normal(150, 8)$$ 
This is 8, thereby we change our sd in the alpha to from 10 to 8. 

### Bonus! 😃

#### Problem 4H1
The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals for each of these individuals. That is, fill in the table below, using model-based predictions.
  Individual | expected height | 89% interval weight
1 46.95      | 
2 43.72
3 64.78
4 32.59
5 54.63





```{r}
# first reloading the data:
library(rethinking, tidyverse)
data(Howell1)
d<- Howell1

#now we need to filter out people under 18
d <- d %>% 
    filter(age>=18)

#first make an average weight to standardize
xbar <- mean(d$weight)

#make a model that can predict each height for each weights given the data:
model <- quap(alist(#we are now making a list of priors
    height ~ dnorm(mu, sigma), 
    y <- a + b*(weight-xbar), #xbar is the average weight 
    a ~ dnorm(173.5, 18), #defining the alpha prior for the height - the mean is assumed to be 173.5 (Sara's and I's average height), and 18 is the standard deviation (this means that 173.5 (+-18) covers 65% (2 std.) of all observations) 
    #intercept
    b ~ dlnorm(2,2), #slope -> so how much do we go up in cm for each kg of weight
    sigma ~ dexp(1)
), data = d)


#now we need to get values our of it - here we use link:
# link samples from the posterior distribution for each case in the data 
#the weights will use for data:
weight <- c(46.95, 43.72, 64.78, 32.59, 54.63)
weight_data <- data.frame(weight)

y <- link(model, data=weight_data, n = 1000)
y <- data.frame(y)#this now gives us a 1000 (default) samples for each weight. 


# finding the average heights for all of the posterior samples;
y1 <- mean(y$X1)
y2 <- mean(y$X2)
y3 <- mean(y$X3)
y4 <- mean(y$X4)
y5 <- mean(y$X5)

ys <- c(y1, y2, y3, y4, y5) #mean height - mu heights 

height_weight_data <- data.frame(ys, weight)
height_weight_data 

```

```{r}
#the same could have been achieved by:
mu_height <- apply(y,2,mean)
mu_height
```


```{r}
#this is now the 
mu.PI <- apply( y , 2 , PI , prob=0.89)
mu.PI 
```

*THIS IS NOW THE PROBABILITY INTERVAL :D* 





